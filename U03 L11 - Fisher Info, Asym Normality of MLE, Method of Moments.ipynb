{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 3 Lecture 11: Fisher Information, Asymptotic Normality of MLE, Method of Moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Information\n",
    "\n",
    "Fisher Information is a way of measuring the amount of information each i.i.d. variable $X_i$ carries about an unknown parameter $\\theta_j$ in the distribution of $X \\sim P(\\theta_0, \\cdots, \\theta_n)$. In other words, it's the amount of information each sample provides on a parameter of the sample's distribution. Geometrically, it tells you, on average, how curved the log-likelihood of $(x_1, \\theta)$ is.\n",
    "\n",
    "Fisher Information is calculated using likelihood function of the pdf of $X$ with one sample. The likelihood function with one sample is just the pdf.\n",
    "\n",
    "$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, $\\theta := \\sigma^2$\n",
    "\n",
    "$L(\\theta | X) = \\dfrac{1}{\\sqrt{2 \\pi \\theta}} \\exp(-\\frac{1}{2 \\theta} (X - \\mu)^2)$\n",
    "\n",
    "To get to the Fisher Information, we first take the derivative of the log-likelihood, just like in getting $\\hat{\\theta}_{MLE}$. \n",
    "\n",
    "$\\dfrac{\\partial \\mathcal{ln}(L)}{\\partial \\theta} = \\dfrac{-\\theta + X^2 - 2 \\mu X + \\mu^2}{2 \\theta^2}$\n",
    "\n",
    "From here, there are two ways to get the Fisher Information. We can get the variance of the first derivative, or we can just get the negative expectation of the second derivative.\n",
    "\n",
    "$\\dfrac{\\partial^2 \\mathcal{ln}(L)}{\\partial \\theta^2} = \\dfrac{\\theta - 2 X^2 + 4 \\mu X - 2 \\mu^2}{2 \\theta^3}$\n",
    "\n",
    "Now we get the negative expected value, where $\\mathbb{E}[X] = \\mu$ and $\\mathbb{E}[X^2] = \\mu^2 + \\sigma^2$, so...\n",
    "\n",
    "$I(\\theta) = -\\mathbb{E}[\\dfrac{\\partial^2 \\mathcal{log}(L)}{\\partial \\theta^2}] = -\\dfrac{\\theta - 2 (\\mu^2 + \\theta) + 4 (\\mu) (\\mu) - 2 (\\mu)^2}{2 \\theta^3} = \\dfrac{1}{2 \\theta^2} = \\dfrac{1}{2 \\sigma^4} = (Var(\\hat{\\sigma^2}_{MLE}))^{-1}$\n",
    "\n",
    "Note this works no matter what $\\mu$ is. Now, we know the asymptotic variance of the variance estimator is...\n",
    "\n",
    "$Var(\\hat{\\sigma^2}_{MLE}) = 2 \\sigma^4$\n",
    "\n",
    "And we know that of all estimators, the Maximum Likelihood Estimator has the smallest variance.\n",
    "\n",
    "The complete Fisher Information for the model is the covariance matrix:\n",
    "\n",
    "$I_L(\\mu, \\sigma^2) = \\begin{pmatrix}\n",
    "-\\mathbb{E}[\\dfrac{\\partial^2 \\mathcal{log}(L)}{\\partial \\mu^2}] & -\\mathbb{E}[\\dfrac{\\partial^2 \\mathcal{log}(L)}{\\partial \\mu \\partial \\sigma^2}] \\\\\n",
    "-\\mathbb{E}[\\dfrac{\\partial^2 \\mathcal{log}(L)}{\\partial \\sigma^2 \\mu}] & -\\mathbb{E}[\\dfrac{\\partial^2 \\mathcal{log}(L)}{\\partial (\\sigma^2)^2}]\n",
    "\\end{pmatrix} = \\begin {pmatrix}\n",
    "\\dfrac{1}{\\sigma^2} & 0 \\\\\n",
    "0 & \\dfrac{1}{2 \\sigma^4}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Some ambiguous, probably complicated conditions have to be met for the inverse of the Fisher Information to be equal to the variance of MLE. So far it's always true.\n",
    "\n",
    "### Parameter of a distribution depending on parameter of another distribution\n",
    "\n",
    "If variable $Y$ has a parameter that depends on a parameter of variable $X$...\n",
    "\n",
    "$X \\sim Exp(\\lambda)$\n",
    "\n",
    "$Y \\sim Ber(p(\\lambda))$\n",
    "\n",
    "where $z$ is some threshold on the exponential distribution such that everything $> z$ is recorded as a $1$ and everything $< z$ gets a $0$, it makes sense that we can get some information about $\\lambda$ from $p$. It's important, then, that we focus on $\\lambda$ in the process of looking for information about it.\n",
    "\n",
    "$L(\\lambda | Y) = e^{-\\lambda z Y}(1 - e^{-\\lambda z})^{1 - Y}$\n",
    "\n",
    "$\\mathscr{l} = \\mathcal{ln}(L(\\lambda | Y)) = -\\lambda z Y + (1 - Y) ln(1 - e^{-\\lambda z})$\n",
    "\n",
    "Since we want information about $\\lambda$, we take the derivative with respect to it, rather than taking the derivative of $p = e^{-\\lambda z}$. With a little more work, this eventually yields $I(\\lambda)$. It's also possible to use the delta method here, though that's definitely not the easiest way to get the asymptotic variance of $\\hat{\\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic Normality of MLE\n",
    "\n",
    "MLE estimators are asympotically normal *under some regularity conditions*. If an MLE estimator is asymptotically normal, that means (among many things) that we can apply the delta method to find the variance of $g(\\hat{\\theta}_{MLE})$.\n",
    "\n",
    "$\\sqrt{n} (\\hat{\\theta}_{MLE} - \\theta) \\to \\mathcal{N}(0, \\sigma^2)$\n",
    "\n",
    "means\n",
    "\n",
    "$\\sqrt{n} (g(\\hat{\\theta}_{MLE}) - g(\\theta)) \\to \\mathcal{N}(0, g\\prime(E[X])^2\\sigma^2)$\n",
    "\n",
    "and, multivariate\n",
    "\n",
    "$\\sqrt{n} (g(\\hat{\\theta}_{MLE}) - g(\\theta)) \\to \\mathcal{N}(0, \\nabla g(\\theta)^T\\Sigma_X \\nabla g(\\theta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Moments\n",
    "\n",
    "$E[X^k]$ is the $k$th moment of an R.V. $X$. \n",
    "\n",
    "### $\\frac{1}{n} \\sum_{i=1}^n X_i^k = \\overline{X^k}_n$ \n",
    "\n",
    "is the estimator for the $k$th moment. A simple combination of method of moments estimators can be used to estimate a parameter, such as in $\\overline{X^2}_n - \\overline{X_n}^2 \\to \\sigma^2$ of a normal.\n",
    "\n",
    "The formulas for the $k$th moment are, of course\n",
    "\n",
    "$\\int_x x^k f_X(x) dx = \\mathbb{E}[X^k]$\n",
    "\n",
    "$\\sum_x x^k f_X(x) = \\mathbb{E}[X^k]$\n",
    "\n",
    "\n",
    "### Recovering parameters from moments\n",
    "\n",
    "If $\\psi$ is a function that maps the parameters of $X$ to a list of moments of $X$ \n",
    "\n",
    "- $\\psi(\\theta^{(X)}_1, \\cdots, \\theta^{(X)}_n) = (m_1, m_2, \\cdots) = (\\mathbb{E}[X], \\mathbb{E}[X^2], \\cdots)$,\n",
    "\n",
    "And if $\\psi$ is one-to-one, meaning given the output of $\\psi$, there could only be one set of parameters that generated that output, then\n",
    "\n",
    "- $\\psi^{-1}(m_1, m_2, \\cdots) = (\\theta^{(X)}_1, \\cdots, \\theta^{(X)}_n)$\n",
    "\n",
    "e.g. $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "\n",
    "$\\psi_{(1, 2)}(\\mu, \\sigma^2) = (\\mu, \\mu^2 + \\sigma^2)$\n",
    "\n",
    "$\\psi^{-1}(\\mu, \\mu^2 + \\sigma^2) = (\\mu, \\sigma^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE vs MoM\n",
    "\n",
    "- MoM provides consistent, but often biased estimators. Typically easier to compute than MLE (though in this class MLE more often than not has been MoM). Doesn't give a good result if the model isn't well specified since noisy data can throw a mean estimation far away from the true mean.\n",
    "- MLE is more accurate by measure of quadratic risk, and still gives good results if the model isn't well specified."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
