{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 3 Lecture 9 - Introduction to Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Moments\n",
    "\n",
    "$E[X^k]$ is the $k$th moment of an R.V. $X$. \n",
    "\n",
    "### $\\frac{1}{n} \\sum_{i=1}^n X_i^k$ \n",
    "\n",
    "is the estimator for the $k$th moment. A combination of method of moments estimators can be used to estimate a parameter, such as in $\\sigma^2 = E[X^2] - (E[X])^2$. \n",
    "\n",
    "Provides consistent, but often biased estimators.\n",
    "\n",
    "## Maximum Likelihood Estimation\n",
    "\n",
    "- $L_n := L(\\theta | X_1, ..., X_n) = \\prod_{i=1}^n f(X_i | \\theta)$ \n",
    "\n",
    "takes a sample $\\{X_1,...,X_n\\}$ and a parameter $\\theta$ and gives the likelihood the sample was created from a distribution with PDF/PMF $f$. Geometrically, it finds the maximum, or peak, of the likelihood function.\n",
    "\n",
    "- Requires iid.\n",
    "\n",
    "- $\\hat{\\theta}_{MLE}$ is the $\\theta$ that maximizes $L_n$ $\\forall \\theta \\in \\Theta$.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Bernoulli: $p^{\\sum x_i}(1-p)^{n - \\sum x_i}$\n",
    "- Poisson: $\\dfrac{\\lambda^{\\sum x_i}}{\\prod x_i!}e^{-n \\lambda}$\n",
    "- Gaussian: $\\dfrac{1}{(\\sigma \\sqrt{2 \\pi})^n} exp(- \\dfrac{1}{2 \\sigma^2} \\sum (x_i - \\mu)^2)$\n",
    "- Exponential: $\\lambda^n exp(- \\lambda \\sum x_i)$\n",
    "\n",
    "### Estimator w/ Indicator \n",
    "\n",
    "An MLE estimator might have an indicator attatched to it, such as with\n",
    "\n",
    "$\\lambda^n exp(- \\lambda \\sum x_i) \\mathbb{1}(x_i > 0) = \\lambda^n exp(- \\lambda \\sum x_i) \\mathbb{1}(min(x_i) > 0)$\n",
    "\n",
    "However, since in this class the model is always well-specified, $x_i$ samples will always be $> 0$, making the indicator unnecessary. Only indicators that depend on the parameter are relevant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concave, Convex\n",
    "\n",
    "If the second derivative of $L_n$ is negative (resp. positive) over all of $\\theta$, the function is strictly concave (resp. strictly convex). If $L_n$ is strictly concave and the argmax is finite, setting $\\frac{d}{d\\theta}[L_n] = 0$ finds the maximum. Then, solving for $\\theta$ gives $\\hat{\\theta}_{MLE}$.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>concave</th>\n",
    "        <th>$g \\prime \\prime (x) \\leq 0$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>strictly concave</th>\n",
    "        <th>$g \\prime \\prime (x) < 0$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>convex</th> \n",
    "        <th>$g \\prime \\prime (x) \\geq 0$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>strictly convex</th>\n",
    "        <th>$g \\prime \\prime (x) > 0$</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood\n",
    "\n",
    "Maximizing $ln(L_n)$ will also maximize $L_n$, since $ln$ is a monotonic function.\n",
    "\n",
    "If setting $\\frac{d}{d\\theta}[L_n] = 0$ finds that maximum (as it will for a strictly concave function, resp. min for convex), so does setting $\\frac{d}{d\\theta}[ln(L_n)] = 0$. And, since taking the derivative of a product is harder than taking the derivative of a sum, in this case using $ln$ is the preferred method of finding $\\hat{\\theta}_{MLE}$. Thinking geometrically might also lead to an intuitive answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "The gradient of a function is the multi-variable version of a derivative, or vector of partial derivatives. Geometrically, you can think of it as being the partial contribution of each variable to the direction of the function, just as the derivative does with one variable.\n",
    "\n",
    "$f(\\theta) = c_1 \\theta_1^2 + c_2 \\theta_2^3 + c_3 \\theta_3^2$\n",
    "\n",
    "$\\nabla f = \\begin{pmatrix}\n",
    "\\frac{df}{d\\theta_1} \\\\\n",
    "\\frac{df}{d\\theta_2} \\\\\n",
    "\\frac{df}{d\\theta_3}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "$= \\begin{pmatrix}\n",
    "2 c_1  \\theta_1 \\\\\n",
    "3  c_2  \\theta_2^2 \\\\\n",
    "2  c_3  \\theta_3\n",
    "\\end{pmatrix}$\n",
    "\n",
    "If a function is strictly concave and all elements of the gradient are zero at a point, that point represents the maximum of the function. The same goes for being strictly convex and finding the minimum. \n",
    "\n",
    "Functions are sometimes more complicated. The most popular algorithm to search for the global minimum error of a prediction function is called 'gradient descent'. The process is like rolling a ball downhill, looking for the deepest pit on a hilly surface, with a blindfold on.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*f9a162GhpMbiTVTAua_lLQ.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian Matrix\n",
    "\n",
    "The hessian is a square matrix of second-order partial derivatives of a function. Where the gradient describes local contribution to direction, the hessian describes local curvature.\n",
    "\n",
    "$H_{i, j} = \\dfrac{d^2 f}{d \\theta_i d \\theta_j}$\n",
    "\n",
    "$\\nabla f = \\begin{pmatrix}\n",
    "2 c_1  \\theta_1 \\\\\n",
    "3  c_2  \\theta_2^2 \\\\\n",
    "2  c_3  \\theta_3\n",
    "\\end{pmatrix}$\n",
    "\n",
    "$H_f = \\begin{bmatrix}\n",
    "2 c_1 & 0 & 0 \\\\\n",
    "0 & 6 c_2 \\theta & 0 \\\\\n",
    "0 & 0 & 2 c_3\n",
    "\\end{bmatrix}$\n",
    "\n",
    "#### Positive & Negative // Definite & Semi-Definite // Concave & Convex\n",
    "\n",
    "A $d$ x $d$ matrix $H$ is positive semi-definite (resp. negative semi-definite) if...\n",
    "\n",
    "$x^THx \\geq (resp. \\leq)$ $0$  $\\forall x \\in \\mathbb{R}^d$\n",
    "\n",
    "... where $x$ isn't a vector of all zeroes. Another way to think of it is that if you added any of the elements together in any combination, the result must be positive (resp. negative) or zero.\n",
    "\n",
    "- $\\leq 0$ $\\to$ negative semi-definite $\\to$ concave\n",
    "- $ < 0$ $\\to$ negative definite $\\to$ strictly concave \n",
    "- $\\geq 0$ $\\to$ positive semi-definite $\\to$ convex\n",
    "- $ > 0$ $\\to$ positive definite $\\to$ strictly convex \n",
    "\n",
    "If a matrix is definite, it is also semi-definite by definition.\n",
    "\n",
    "#### Determinant and Trace\n",
    "\n",
    "Another method of finding convexity/concavity if the matrix is 2x2 and symmetric:\n",
    "\n",
    "$H = \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- $det(H) = ad - bc$\n",
    "- $tr(H) = a + d$\n",
    "\n",
    "... where the trace is generally the sum of elements on the main diagonal.\n",
    "\n",
    "- $tr(H) \\leq 0$\n",
    "- $det(H) \\geq 0$\n",
    "\n",
    "would mean a matrix is negative semi-definite. \n",
    "\n",
    "I personally don't know if this works for larger matrices, though I know finding the determinant is a more complicated process without these conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
