{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 3 Lecture 8 - Distance Measures Between Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Variation (TV) Distance\n",
    "\n",
    "Gives us a notion of distance between some distributions.\n",
    "\n",
    "- $TV$ is symmetric, positive, definite, and it satisfies the triangle inequality.\n",
    "\n",
    "For two probability distributions $\\mathbb{P}_{\\theta}$, $\\mathbb{P}_{\\theta^{\\prime}}$ over the union of their supports $E$, $\\mathbb{P}_{\\theta}$ and $\\mathbb{P}_{\\theta^{\\prime}}$ are close by measure of TV if\n",
    "\n",
    "$|\\mathbb{P}_{\\theta}(A) - \\mathbb{P}_{\\theta^{\\prime}}(A)|$ is small $\\forall A \\subset E$\n",
    "\n",
    "So,\n",
    "\n",
    "- $TV(\\mathbb{P}_{\\theta}, \\mathbb{P}_{\\theta^{\\prime}}) = max_{A \\subset E} |\\mathbb{P}_{\\theta}(A) - \\mathbb{P}_{\\theta^{\\prime}}(A)|$\n",
    "\n",
    "with formulas\n",
    "\n",
    "- $TV(\\mathbb{P}_{\\theta}, \\mathbb{P}_{\\theta^{\\prime}}) = \\frac{1}{2} \\sum_{x \\in E} |p_{\\theta}(x) - p_{\\theta^{\\prime}}(x)|$  \n",
    "- $TV(\\mathbb{P}_{\\theta}, \\mathbb{P}_{\\theta^{\\prime}}) = \\frac{1}{2} \\int_{x \\in E} |f_{\\theta}(x) - f_{\\theta^{\\prime}}(x)| dx$  \n",
    "\n",
    "outputs a value $\\in [0, 1]$\n",
    "\n",
    "$TV$ doesn't provide a notion of distance between most distributions, since $TV($continuous, discrete$)$, $TV($continuous distributions with no overlap$)$, and discrete cases like $X$ and $X + \\epsilon$, $\\epsilon \\notin \\{-1, 0, 1\\}$ all have $TV=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler (KL) Divergence\n",
    "\n",
    "Gives us a notion of divergence of one distribution from another. Also known as 'relative entropy.'\n",
    "\n",
    "- $KL$ is not symmetric, positive, definite, and does not satisfy the triangle inequality.\n",
    "\n",
    "For a fixed, unknown $\\theta^*$, KL-Divergence has formulas\n",
    "\n",
    "- $KL(\\mathbb{P}_{\\theta^*}, \\mathbb{P}_{\\theta}) = \\sum_{x \\in E} p_{\\theta^*}(x) \\cdot ln(\\frac{p_{\\theta^*}(x)}{p_{\\theta}(x)})$\n",
    "\n",
    "- $KL(\\mathbb{P}_{\\theta^*}, \\mathbb{P}_{\\theta}) = \\int_{x \\in E} f_{\\theta^*}(x) \\cdot ln(\\frac{f_{\\theta^*}(x)}{f_{\\theta}(x)})$\n",
    "\n",
    "$= \\mathbb{E}_{\\theta^*} [ln(\\frac{p_{\\theta^*}(X)}{p_{\\theta}(X)})]$\n",
    "\n",
    "$= \\mathbb{E}_{\\theta^*}[ln(p_\\theta^*(X)] - \\mathbb{E}_{\\theta^*}[ln(p_{\\theta}(X)]$\n",
    "\n",
    "The first term in this difference is a constant, so minimizing this difference means minimizing\n",
    "\n",
    "- $\\hat{KL}(\\mathbb{P}_{\\theta^*}, \\mathbb{P}_{\\theta}) = c - \\frac{1}{n} \\sum ln(p_{\\theta}(X_i))$\n",
    "\n",
    "given $\\frac{1}{n} \\sum h(X_i) \\xrightarrow[]{D} \\mathbb{E}_{\\theta^*}[h(X)]$ for all $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Principle\n",
    "\n",
    "here $\\to$ means 'if $\\theta$ satisfies the previous, it equivalently satisfies this'\n",
    "\n",
    "$min_{\\theta \\in \\Theta} \\hat{KL}(\\mathbb{P}_{\\theta^*}, \\mathbb{P}_{\\theta})$\n",
    "\n",
    "$\\to min_{\\theta \\in \\Theta} - \\frac{1}{n} \\sum ln(p_{\\theta}(X_i))$\n",
    "\n",
    "$\\to max_{\\theta \\in \\Theta} \\frac{1}{n} \\sum ln(p_{\\theta}(X_i))$\n",
    "\n",
    "$\\to max_{\\theta \\in \\Theta} ln(\\prod p_{\\theta}(X_i))$\n",
    "\n",
    "$\\to max_{\\theta \\in \\Theta} \\prod p_{\\theta}(X_i)$\n",
    "\n",
    "This is the maximum of the likelihood function $L_n(\\theta | X_1, ..., X_n)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
